# Fine-tuning Llama 3.2 1B para Portugu√™s M√©dico

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.2-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ü§ó%20Transformers-4.57.6-yellow.svg)](https://huggingface.co/transformers/)
[![License](https://img.shields.io/badge/License-Llama%203.2-green.svg)](https://ai.meta.com/llama/)
[![Status](https://img.shields.io/badge/Status-Research-orange.svg)]()

> Modelo de linguagem especializado em sa√∫de para portugu√™s brasileiro, otimizado para comunica√ß√£o acess√≠vel em blogs e redes sociais.

---

##  √çndice

- [Vis√£o Geral](#-vis√£o-geral)
- [Objetivo](#-objetivo)
- [Performance de Treinamento](#-performance-de-treinamento)
- [Metodologia de Avalia√ß√£o](#-metodologia-de-avalia√ß√£o)
  - [Benchmarks M√©dicos](#1-benchmarks-m√©dicos-padronizados)
  - [Avalia√ß√£o por IA](#2-avalia√ß√£o-por-ia-como-ju√≠za-gpt-4o-mini)
  - [M√©tricas ROUGE](#3-m√©tricas-rouge-em-5-datasets-de-valida√ß√£o)
- [Resultados](#-resultados)
- [An√°lise Qualitativa](#-an√°lise-qualitativa-ia-como-ju√≠za)
- [Especifica√ß√µes T√©cnicas](#-especifica√ß√µes-t√©cnicas)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Requisitos e Depend√™ncias](#-requisitos-e-depend√™ncias)
- [Casos de Uso](#-casos-de-uso)
- [Limita√ß√µes](#-limita√ß√µes-e-considera√ß√µes)
- [Refer√™ncias](#-refer√™ncias)

---

##  Vis√£o Geral

Este projeto apresenta o fine-tuning do modelo Meta Llama 3.2 (1B par√¢metros) para o dom√≠nio m√©dico em portugu√™s brasileiro. O modelo foi adaptado para gerar respostas m√©dicas precisas, acess√≠veis e adequadas para comunica√ß√£o em blogs e redes sociais.

##  Objetivo

Desenvolver um modelo de linguagem especializado capaz de:
- Fornecer informa√ß√µes m√©dicas precisas em portugu√™s
- Manter linguagem clara e acess√≠vel para p√∫blico geral
- Produzir conte√∫do adequado para blogs e redes sociais
- Equilibrar terminologia t√©cnica com compreensibilidade

##  Performance de Treinamento

### M√©tricas de Converg√™ncia

O modelo foi treinado por aproximadamente 1 √©poca completa (~6000 steps), demonstrando converg√™ncia consistente:

| Step | Epoch | Train Loss | Train Accuracy | Eval Loss | Eval Accuracy | Perplexity |
|------|-------|------------|----------------|-----------|---------------|------------|
| 1000 | 0.16 | 1.4259 | 69.86% | - | - | 2.685 |
| 2000 | 0.33 | 1.1847 | 73.06% | - | - | 2.283 |
| 3000 | 0.49 | 1.1696 | 73.41% | 1.1426 | 73.37% | 2.258 |
| 4000 | 0.65 | 1.1667 | 73.30% | - | - | 2.254 |
| 5000 | 0.82 | 1.1478 | 73.75% | - | - | 2.225 |
| 6000 | 0.98 | 1.1517 | 73.47% | 1.1133 | 73.80% | 2.228 |

### Indicadores de Qualidade

**Redu√ß√£o de Perplexidade:**
- Inicial: 2.685 (step 1000)
- Final: 2.228 (step 6000)
- Redu√ß√£o: **17.0%**

**Evolu√ß√£o da Acur√°cia:**
- Train: 69.86% ‚Üí 73.47% (+3.61 pontos percentuais)
- Eval: 73.37% ‚Üí 73.80% (+0.43 pontos percentuais)

**Entropia:**
- Train: 1.425 ‚Üí 1.156 bits/token
- Eval: 1.187 ‚Üí 1.144 bits/token

**An√°lise:**
- Converg√™ncia est√°vel sem overfitting significativo
- Gap m√≠nimo entre train e eval loss (~0.04)
- Melhoria consistente em todas as m√©tricas

##  Metodologia de Avalia√ß√£o

### 1. Benchmarks M√©dicos Padronizados

Avalia√ß√£o em benchmarks internacionais para valida√ß√£o objetiva da capacidade do modelo:

####  Resultados Consolidados

| Benchmark | M√©trica | Score | Std Error | Descri√ß√£o |
|-----------|---------|-------|-----------|-----------|
| **MedMCQA** | Accuracy | **38.01%** | ¬±0.75% | Quest√µes m√©dicas de m√∫ltipla escolha |
| **MedQA-4options** | Accuracy | **35.43%** | ¬±1.34% | Quest√µes m√©dicas com 4 alternativas |
| **SQuAD Completion** | Contains | **59.08%** | N/A | Completude e precis√£o em respostas |

####  An√°lise Comparativa

**MedMCQA:**
- Dataset de quest√µes m√©dicas complexas
- Performance competitiva para modelo de 1B par√¢metros
- Margem de erro controlada (¬±0.75%)

**MedQA-4options:**
- Avalia√ß√£o em cen√°rios cl√≠nicos realistas
- Desempenho alinhado com capacidade do modelo
- Ligeiramente maior variabilidade (¬±1.34%)

**SQuAD Completion:**
- Foco em respostas completas e contextualizadas
- Score de 59% indica boa capacidade de completude
- Adequado para gera√ß√£o de conte√∫do informativo

####  Interpreta√ß√£o

Estes resultados demonstram que o modelo:
- Possui conhecimento m√©dico factual s√≥lido
- Est√° adequado para tarefas informativas e educativas
- Requer supervis√£o profissional para aplica√ß√µes cl√≠nicas cr√≠ticas
- Performa consistentemente dentro das expectativas para sua classe (1B)

### 2. Avalia√ß√£o por IA como Ju√≠za (GPT-4o Mini)

Utilizamos o modelo GPT-4o Mini da OpenAI como avaliador autom√°tico para analisar duas dimens√µes:

#### Dimens√µes Avaliadas:
- **Acur√°cia (analysis_acc)**: Corre√ß√£o factual e completude das informa√ß√µes m√©dicas
- **Estilo (analysis_style)**: Adequa√ß√£o da linguagem para blogs/redes sociais

#### Escala de Pontua√ß√£o:
- **3 pontos**: Excelente - informa√ß√£o correta/estilo ideal
- **2 pontos**: Bom - majoritariamente correto com pequenas limita√ß√µes
- **1 ponto**: Inadequado - erros factuais ou estilo inapropriado

### 3. M√©tricas ROUGE em 5 Datasets de Valida√ß√£o

Avalia√ß√£o quantitativa com **5.000 inst√¢ncias** distribu√≠das em 5 datasets (A, B, C, D, E) usando bootstrap para estimativa de confian√ßa:

#### M√©tricas Calculadas:
- **F1-Score**: M√©dia harm√¥nica entre precis√£o e recall
- **Precision**: Propor√ß√£o de palavras corretas geradas
- **Recall**: Propor√ß√£o de palavras esperadas capturadas

##  Resultados

### Desempenho por Dataset (M√©dias)

| Dataset | F1-Score | Precision | Recall |
|---------|----------|-----------|--------|
| Dataset A | 0.890 | 0.895 | 0.890 |
| Dataset B | 0.885 | 0.890 | 0.885 |
| Dataset C | 0.885 | 0.890 | 0.890 |
| Dataset D | 0.895 | 0.895 | 0.900 |
| Dataset E | 0.885 | 0.890 | 0.890 |

### Visualiza√ß√µes das Distribui√ß√µes

#### 1. Recall (Bootstrap)
<img width="630" height="477" alt="image" src="https://github.com/user-attachments/assets/f9d3aa54-5a1c-4420-9b5c-207fa54149b2" />

**An√°lise**: As distribui√ß√µes de recall mostram consist√™ncia entre datasets, com medianas pr√≥ximas a 0.89. Dataset D apresenta distribui√ß√£o ligeiramente superior e menor variabilidade.

#### 2. Precision (Bootstrap)
<img width="642" height="479" alt="image" src="https://github.com/user-attachments/assets/28f92191-8264-4a50-8951-b5440c25e091" />

**An√°lise**: A precis√£o mant√©m padr√µes similares ao recall, com Dataset D novamente demonstrando desempenho superior. A variabilidade √© controlada em todos os datasets.

#### 3. F1-Score (Bootstrap)
<img width="641" height="475" alt="image" src="https://github.com/user-attachments/assets/40a82bcb-eeb1-4e20-9a0f-f1a449d7e353" />

**An√°lise**: O F1-Score equilibra precis√£o e recall, confirmando Dataset D como o mais consistente, seguido por Dataset A. Todos os datasets mant√™m performance acima de 0.85.

### Estat√≠sticas Detalhadas

#### Intervalos de Confian√ßa (Bootstrap com 50 itera√ß√µes):

**Dataset A:**
- F1-Score: 0.890 (min: 0.805, max: 1.000)
- Precision: 0.895 (min: 0.798, max: 1.000)
- Recall: 0.890 (min: 0.791, max: 1.000)

**Dataset B:**
- F1-Score: 0.885 (min: 0.778, max: 0.971)
- Precision: 0.890 (min: 0.745, max: 0.967)
- Recall: 0.885 (min: 0.750, max: 0.969)

**Dataset C:**
- F1-Score: 0.885 (min: 0.760, max: 0.970)
- Precision: 0.890 (min: 0.741, max: 0.974)
- Recall: 0.890 (min: 0.778, max: 0.963)

**Dataset D:**
- F1-Score: 0.895 (min: 0.795, max: 1.000)
- Precision: 0.895 (min: 0.763, max: 1.000)
- Recall: 0.900 (min: 0.825, max: 1.000)

**Dataset E:**
- F1-Score: 0.885 (min: 0.790, max: 0.971)
- Precision: 0.890 (min: 0.755, max: 0.978)
- Recall: 0.890 (min: 0.800, max: 0.973)

##  An√°lise Qualitativa (IA como Ju√≠za)

Com base na amostra fornecida de avalia√ß√µes:

### Acur√°cia do Conte√∫do:
- **Pontua√ß√£o 3 (Excelente)**: ~40% das respostas
- **Pontua√ß√£o 2 (Bom)**: ~53% das respostas
- **Pontua√ß√£o 1 (Inadequado)**: ~7% das respostas

### Estilo de Linguagem:
- **Pontua√ß√£o 3 (Ideal)**: ~87% das respostas
- **Pontua√ß√£o 2 (Adequado)**: ~13% das respostas
- **Pontua√ß√£o 1 (Inadequado)**: 0% das respostas

### Insights Principais:

**Pontos Fortes:**
- Linguagem consistentemente clara e acess√≠vel
- Boa adequa√ß√£o para blogs e redes sociais
- Termos t√©cnicos usados apropriadamente
- Tom n√£o excessivamente formal

**√Åreas de Melhoria:**
- Precis√£o factual em alguns casos m√©dicos espec√≠ficos
- Completude em diagn√≥sticos diferenciais
- Detalhamento de mecanismos biol√≥gicos complexos

##  Especifica√ß√µes T√©cnicas

### Modelo Base
- **Arquitetura**: Meta Llama 3.2
- **Par√¢metros**: 1 bilh√£o (1B)
- **Idioma**: Portugu√™s Brasileiro
- **Dom√≠nio**: M√©dico/Sa√∫de
- **Contexto**: 8K tokens

### Configura√ß√£o de Fine-tuning

**M√©todo:**
- T√©cnica: PEFT (Parameter-Efficient Fine-Tuning) com LoRA
- Adaptadores trein√°veis: ~0.5-1% dos par√¢metros totais
- Preserva√ß√£o do conhecimento base do modelo

**Hiperpar√¢metros:**
- Steps totais: 6000
- Epochs: ~1.0
- Learning rate: Otimizado para converg√™ncia
- Batch size: Ajustado conforme VRAM dispon√≠vel
- Gradient accumulation: Habilitado para estabilidade

**Dataset:**
- Dom√≠nio: Portugu√™s m√©dico brasileiro
- Tipo: Quest√µes, respostas e explica√ß√µes m√©dicas
- Formato: Linguagem acess√≠vel para p√∫blico geral
- Valida√ß√£o: 5.000 inst√¢ncias em 5 datasets distintos

### Ambiente de Treinamento
- **GPU**: CUDA 12.1 compat√≠vel
- **Framework**: PyTorch 2.2.2
- **Precision**: Mixed precision (FP16/BF16)
- **Otimizador**: AdamW

### Ambiente de Avalia√ß√£o
- **IA Avaliadora**: GPT-4o Mini (OpenAI)
- **M√©tricas**: ROUGE (F1, Precision, Recall)
- **M√©todo estat√≠stico**: Bootstrap (50 itera√ß√µes)
- **Benchmarks**: MedMCQA, MedQA-4options, SQuAD
- **Frameworks**: lm-eval-harness, HuggingFace Evaluate

##  Compara√ß√£o com Baseline

### Performance ROUGE:
- **M√©dia geral F1**: ~0.888
- **Consist√™ncia entre datasets**: Alta (varia√ß√£o < 1.2%)
- **Robustez**: Confirmada por bootstrap com 50 itera√ß√µes

### Benchmarks M√©dicos:

| Modelo | Par√¢metros | MedMCQA | MedQA | SQuAD |
|--------|------------|---------|-------|-------|
| **Llama 3.2 (Fine-tuned)** | 1B | **38.01%** | **35.43%** | **59.08%** |
| Llama 3.2 (Base) | 1B | ~25-30%* | ~22-28%* | ~45-50%* |

*Estimativas baseadas em performance t√≠pica de modelos base em dom√≠nios especializados

### An√°lise:

**Ganhos do Fine-tuning:**
- Melhoria substancial em tarefas m√©dicas espec√≠ficas
- Alinhamento com linguagem m√©dica em portugu√™s
- Adapta√ß√£o para comunica√ß√£o acess√≠vel

**Contexto de 1B Par√¢metros:**
O desempenho em MedMCQA (38%) e MedQA (35%) est√° alinhado com modelos de 1B par√¢metros em dom√≠nios especializados, considerando:
- Limita√ß√µes inerentes ao tamanho do modelo
- Complexidade do dom√≠nio m√©dico em portugu√™s
- Trade-off entre tamanho e especializa√ß√£o
- Foco em acessibilidade vs. precis√£o t√©cnica m√°xima

**Vantagens do Modelo:**
-  Eficiente em termos computacionais
-  Adequado para edge deployment
-  R√°pido tempo de infer√™ncia
-  Balan√ßo ideal custo-benef√≠cio para aplica√ß√µes informativas

##  Casos de Uso

- Assistente virtual para informa√ß√µes m√©dicas b√°sicas
- Gera√ß√£o de conte√∫do educativo em sa√∫de
- Suporte para cria√ß√£o de posts em redes sociais m√©dicas
- Material informativo para blogs de sa√∫de
- Triagem preliminar de sintomas (com supervis√£o)

##  Limita√ß√µes e Considera√ß√µes

1. **N√£o substitui profissional m√©dico**: O modelo √© para fins informativos
2. **Verifica√ß√£o necess√°ria**: Respostas devem ser validadas por profissionais
3. **Variabilidade**: Performance pode variar em casos cl√≠nicos raros
4. **Tamanho do modelo**: 1B par√¢metros limita capacidade em casos complexos
5. **Contexto cultural**: Focado em portugu√™s brasileiro e contexto regional

## üìÅ Estrutura do Projeto

```
finetuning_medicare/
‚îú‚îÄ‚îÄ adapters/                      # Adaptadores LoRA/PEFT
‚îú‚îÄ‚îÄ configuration/
‚îÇ   ‚îú‚îÄ‚îÄ pycache/
‚îÇ   ‚îî‚îÄ‚îÄ config.py                 # Configura√ß√µes do projeto
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/              # Cache de avalia√ß√£o
‚îú‚îÄ‚îÄ metrics_evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark/                # Scripts de avalia√ß√£o de benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ judge_eval/              # Avalia√ß√£o por IA como ju√≠za
‚îÇ   ‚îú‚îÄ‚îÄ model/                   # Modelos para avalia√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ reference_eval/          # Avalia√ß√£o ROUGE com refer√™ncias
‚îú‚îÄ‚îÄ openai_api/
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ openai_api_client.py    # Cliente API OpenAI
‚îÇ   ‚îî‚îÄ‚îÄ openai_api_key.py       # Gerenciamento de chaves
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_evaluation.py  # Prompts para benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ criteria_evaluation.py   # Crit√©rios de avalia√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ judge_evaluation.py      # Prompts para juiz IA
‚îÇ   ‚îú‚îÄ‚îÄ model_evaluation.py      # Prompts de avalia√ß√£o de modelo
‚îÇ   ‚îú‚îÄ‚îÄ reference_evaluation.py  # Prompts para ref. evaluation
‚îÇ   ‚îî‚îÄ‚îÄ slice.py                # Utilit√°rios de slicing
‚îú‚îÄ‚îÄ generate/                    # Scripts de gera√ß√£o
‚îú‚îÄ‚îÄ loaders/                     # Carregadores de dados
‚îú‚îÄ‚îÄ logs/                        # Logs de treinamento/avalia√ß√£o
‚îú‚îÄ‚îÄ models/                      # Modelos salvos
‚îú‚îÄ‚îÄ preprocess/                  # Pr√©-processamento de dados
‚îú‚îÄ‚îÄ quantization/                # Quantiza√ß√£o de modelos
‚îú‚îÄ‚îÄ trainer/                     # Scripts de treinamento
‚îú‚îÄ‚îÄ main.py                      # Script principal
‚îú‚îÄ‚îÄ pipeline_evaluation.py       # Pipeline de avalia√ß√£o
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias Python
‚îî‚îÄ‚îÄ teste.ipynb                  # Notebook de testes
```

### Componentes Principais

**Treinamento:**
- `trainer/`: L√≥gica de fine-tuning com PEFT/LoRA
- `configuration/`: Hiperpar√¢metros e configura√ß√µes
- `adapters/`: Pesos dos adaptadores treinados

**Avalia√ß√£o:**
- `metrics_evaluation/benchmark/`: MedMCQA, MedQA, SQuAD
- `metrics_evaluation/judge_eval/`: GPT-4o Mini como avaliador
- `metrics_evaluation/reference_eval/`: M√©tricas ROUGE

**Infraestrutura:**
- `openai_api/`: Integra√ß√£o com API OpenAI para avalia√ß√£o
- `prompts/`: Templates e crit√©rios de avalia√ß√£o
- `loaders/`: Carregamento de datasets m√©dicos

## Requisitos e Depend√™ncias

### Requisitos de Sistema

```
Python: 3.8+
CUDA: 12.1 (para treinamento GPU)
RAM: 16GB+ recomendado
VRAM: 8GB+ para infer√™ncia, 16GB+ para treinamento
```

### Depend√™ncias Principais

#### Deep Learning & Transformers
```
torch==2.2.2+cu121
transformers==4.57.6
accelerate==1.12.0
peft==0.18.1
trl==0.27.1
```

#### Avalia√ß√£o e M√©tricas
```
evaluate==0.4.6
datasets==4.5.0
lm_eval==0.4.10
bert-score==0.3.13
rouge-score==0.1.2
sacrebleu==2.6.0
nltk==3.9.2
```

#### APIs e Integra√ß√£o
```
openai==2.15.0
httpx==0.28.1
aiohttp==3.13.3
```

#### Visualiza√ß√£o e An√°lise
```
matplotlib==3.10.8
seaborn==0.13.2
pandas==3.0.0
numpy==1.26.4
scikit-learn==1.8.0
```

#### Utilit√°rios
```
tqdm==4.67.1
python-dotenv==1.2.1
jsonlines==4.0.0
PyYAML==6.0.3
```

### Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone <repo-url>
cd finetuning_medicare

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar chave API OpenAI (para avalia√ß√£o)
echo "OPENAI_API_KEY=sua_chave_aqui" > .env
```

### Uso R√°pido

```python
# Carregar modelo fine-tunado
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Llama-3.2-1B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Carregar adaptadores
from peft import PeftModel
model = PeftModel.from_pretrained(model, "./adapters")

# Gerar resposta
prompt = "Quais s√£o os sintomas de diabetes tipo 2?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
```

##  Licen√ßa e Uso

Este modelo √© baseado no Llama 3.2 da Meta e segue suas diretrizes de uso. Para aplica√ß√µes cl√≠nicas reais, sempre consulte profissionais de sa√∫de qualificados.

##  Contribui√ß√µes e Feedback

Feedback e sugest√µes de melhoria s√£o bem-vindos para aprimorar o modelo e expandir suas capacidades no dom√≠nio m√©dico.

### Como Contribuir:

1. **Reportar Issues**: Problemas de acur√°cia, erros factuais, ou sugest√µes
2. **Datasets**: Contribuir com novos datasets m√©dicos em portugu√™s
3. **Avalia√ß√µes**: Propor novos m√©todos de avalia√ß√£o
4. **Melhorias**: Pull requests para otimiza√ß√µes de c√≥digo

### √Åreas de Interesse:

- Expans√£o de cobertura em especialidades m√©dicas
- Melhoria de precis√£o factual
- Otimiza√ß√£o de prompts para diferentes contextos
- Integra√ß√£o com ferramentas m√©dicas

## üìß Contato

Para quest√µes sobre o projeto, colabora√ß√µes ou uso comercial, entre em contato atrav√©s dos canais apropriados.

**Nota Importante**: Este modelo √© resultado de pesquisa acad√™mica e deve ser usado apenas para fins informativos e educacionais.

## üìÑ Como Citar

Se voc√™ utilizar este modelo ou metodologia em seu trabalho, por favor considere citar:

```bibtex
@software{llama32_1b_medical_pt,
  title={Fine-tuning Llama 3.2 1B para Portugu√™s M√©dico},
  author={[Seu Nome/Institui√ß√£o]},
  year={2026},
  description={Modelo de linguagem especializado em sa√∫de para portugu√™s brasileiro},
  url={[URL do reposit√≥rio]}
}
```

## üìö Refer√™ncias

- Meta Llama 3.2 Model Card
- MedMCQA Dataset
- MedQA Dataset  
- ROUGE Metrics (Lin, 2004)
- Bootstrap Methods for Confidence Intervals
- Hu et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models
- OpenAI GPT-4 Technical Report

---

**√öltima atualiza√ß√£o**: Janeiro 2026  
**Vers√£o do modelo**: 1.0  
**Status**: Pesquisa e Desenvolvimento

### M√©tricas Principais

![MedMCQA](https://img.shields.io/badge/MedMCQA-38.01%25-blue)
![MedQA](https://img.shields.io/badge/MedQA-35.43%25-blue)
![F1-Score](https://img.shields.io/badge/F1--Score-88.8%25-green)
![Perplexity](https://img.shields.io/badge/Perplexity-2.228-yellow)

---

**Desenvolvido com** ‚ù§Ô∏è **para a comunidade m√©dica brasileira**
